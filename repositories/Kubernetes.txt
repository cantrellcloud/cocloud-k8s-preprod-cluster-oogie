To create a linked clone
new-linkclone -vmname VMname

###############################################################################

##IP Addresses

Subnet: 10.0.3.0/22
Gateway: 10.0.0.1
DNS 1: 10.0.1.34
DNS 2: 10.231.1.34


*hosts file on all nodes
vi /etc/hosts

10.0.3.220 kubeadmin.dev.local
10.0.3.221 kubectrl001.dev.local
10.0.3.222 kubectrl002.dev.local
10.0.3.223 kubectrl003.dev.local
10.0.3.224 kubework001.dev.local
10.0.3.225 kubework002.dev.local
10.0.3.226 kubework003.dev.local
10.0.3.229 offline-registry.dev.local

*BHN Newwork
10.1.1.230 sofzudub24041-bhn 443/tcp 53/udp

eNetwork is 10.1.1.0 
Subnet mast is 255.255.255.0
Gateway 10.1.1.1
DNS         4.2.2.2

## Deploy Kubernetes

## Reference Sites
https://www.virtualizationhowto.com/2023/12/how-to-install-kubernetes-in-ubuntu-22-04-with-kubeadm/
https://controlplane.com/community-blog/post/the-complete-kubectl-cheat-sheet
https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises
https://docs.tigera.io/calico/latest/operations/calicoctl/install#install-calicoctl-as-a-kubectl-plugin-on-a-single-host

## Repositories
raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml
github.com/projectcalico/calico/releases/download/v3.29.1/calicoctl-linux-amd64
downloads.tigera.io/ee/binaries/v3.19.4/calicoctl
pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key
download.docker.com/linux/ubuntu/gpg
download.docker.com/linux/ubuntu


## Offline Installation
https://github.com/containerd/containerd/blob/main/docs/getting-started.md
https://gist.github.com/dllud/0820d53477bce57b7f29d7b8f7761268
https://kubernetes.io/releases/download/



##System settings

*Verify the MAC address and product_uuid are unique for every node
You can get the MAC address of the network interfaces using the command ip link or ifconfig -a
The product_uuid can be checked by using the command cat /sys/class/dmi/id/product_uuid

*Update hosts file on nodes
/etc/hosts

10.0.3.221 kubectrl001.dev.local
10.0.3.222 kubectrl002.dev.local
10.0.3.223 kubectrl003.dev.local
10.0.3.224 kubework001.dev.local
10.0.3.225 kubework002.dev.local
10.0.3.226 kubework003.dev.local


*turn off swap
swapoff-a

*turn off swap persistent across reboots
disable swap in --> /etc/fstab


*update apt index and install packages for kubernetes
apt-get update
	#apt-transport-https may be a dummy package; if so, you can skip that package
apt-get install -y apt-transport-https ca-certificates curl gpg

*download public signing key for kubernetes repositories
	#If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command
	#mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

*add kubernetes apt repository
	#This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list

*update apt index and install kubernetes packages
apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl

*enable kubelet service
	#The kubelet will restart every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.
systemctl enable --now kubelet


##Update firewall rules

*enable firewall
ufw enable

*Control Plane rules
ufw allow 22/tcp
ufw allow 6443/tcp
ufw allow 2379/tcp
ufw allow 2380/tcp
ufw allow 8080/tcp
ufw allow 10248/tcp
ufw allow 10250/tcp
ufw allow 10259/tcp
ufw allow 10257/tcp

*Worker Nodes rules
ufw allow 22/tcp
ufw allow 5473/tcp
ufw allow 10250/tcp
ufw allow 10256/tcp
ufw allow 30000:32767/tcp

ufw status verbose

*command alias
vi ~/.bash_aliases
alias k=kubectl
alias kg='kubectl get'
alias kd='kubectl describe'
alias ka='kubectl apply'
alias kdelf='kubectl delete -f'
alias kl='kubectl logs'
alias kgall='kubectl get all -A'
alias n=nerdctl
alias ncstart='nerdctl container start'
alias nirm='nerdctl image rm'
alias nil='nerdctl image ls'
alias ncl='nerdctl container ls'
alias npl='nerdctl pull'
alias nph='nerdctl push' 
source ~/.bash_aliases

*kernel parameters
tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter

tee /etc/sysctl.d/kube.conf <<EOT
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOT

sysctl --system


##Installing Containerd container runtime

*setup Dockerâ€™s apt repository
apt update
apt install ca-certificates curl gnupg
	#may not need this: install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
	#may not need this: chmod a+r /etc/apt/keyrings/docker.gpg

*Add the repository to Apt sources:
deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu noble stable
	#or this way:
echo \
"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
$(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
tee /etc/apt/sources.list.d/docker.list > /dev/null
apt update

*install containerd
apt install containerd.io -y

*configure the system so it starts using systemd as cgroup
containerd config default | tee /etc/containerd/config.toml >/dev/null 2>&1

*vi /etc/containerd/config.toml
...
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true

sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

*setup the service to start automatically and check to make sure it is running
systemctl restart containerd
systemctl enable containerd
systemctl status containerd


##Setting Up Kubernetes Repositories

	#Not needed:
*Pull down the GPG key
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list

*Update the apt package index
apt-get update

	#may not need this:
*subnet.env file
cat <<EOF | tee /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.0.0/16
FLANNEL_MTU=1450
FLANNEL_IPMASQ=true
EOF

sysctl --system
kubeadm config images pull
kubeadm init


*As a regular user
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config


##Modify the kubelet ConfigMap
	#Should be set, run command to verify cgroupDriver: systemd
kubectl edit cm kubelet-config -n kube-system


##Install Calico
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml -O
kubectl create -f custom-resources.yaml

*initialize calico network overlay
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

*may take a few minutes for all nodes in the cluster to spin up all the networking nodes and report ready
watch kubectl get pods -n calico-system

	#Worker nodes may be added at this time

*Configure BGP
	#If planning to use NSX network overlay, BGP is not needed

*Configure NSX overlay
apiVersion: crd.projectcalico.org/v1
kind: IPPool
metadata:
  name: ippool-vxlan-dev-internal-subnets
  namespace: dev-internal
spec:
  allowedUses:
    - Workload
    - Tunnel
  blockSize: 26
  cidr: 192.168.69.0/24
  ipipMode: Always
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: CrossSubnet


apiVersion: crd.projectcalico.org/v1
kind: IPPool
metadata:
  name: ippool-vxlan-dev-external-subnets
  namespace: dev-external
spec:
  allowedUses:
    - Workload
    - Tunnel
  blockSize: 26
  cidr: 192.168.68.0/24
  ipipMode: Always
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: CrossSubnet




##Join nodes to cluster
	#To view the join token:
*kubeadm token create --print-join-command

*on each worker node
mkdir -p $HOME/.kube
**cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

*label nodes
kubectl label node k8dev-wkr01 node-role.kubernetes.io/worker=worker
kubectl label node k8dev-wkr02 node-role.kubernetes.io/worker=worker

------------------------------------------------------------------------------------------------------------------

*create continuous running ubuntu server for cluster maintenance
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dev-kubeadmin
  namespace: dev-internal
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dev-kubeadmin
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: dev-kubeadmin
    spec:
      containers:
        - name: dev-kubeadmin
          image: ubuntu:xenial
          ports:
            - containerPort: 2269
          command:
            - /bin/sh
            - '-c'
            - tail -f /dev/null
			- ["printenv"]
		  args: ["HOSTNAME", "KUBERNETES_PORT"]
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 30

*create test application
apiVersion: apps/v1
kind: Deployment
metadata:
  name: devapp01
  namespace: dev-internal
spec:
  replicas: 1
  selector:
    matchLabels:
      app: devapp01
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: devapp01
    spec:
      containers:
        - name: devapp01
          image: ubuntu:xenial
          ports:
            - containerPort: 2201
          command: ["printenv"]
          args: ["HOSTNAME", "KUBERNETES_PORT"]
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 30


*to bash into a pod
k exec --stdin --tty dev-intdmz-linux-test-65bf85b85d-lnnhw --namespace=dev-external -- /bin/bash


###############################################################################
##
## Load Balancing
##
##https://tamerlan.dev/load-balancing-in-kubernetes-a-step-by-step-guide/
##
##

*this is the preferred deployment model

kube-vip-cloud-provider

+===============================================================================================+
|                                                                                               |
|  +-----------------------------------------------------------------------------------------+  |
|  | INGRESS (kube-vip)                                                                      |  |
|  |                                                                                         |  |
|  |     +----------------+             +----------------+            +-----------------+    |  |
|  |     | foo.domain.tld |             | www.domain.ltd |            | app1.domain.ltd |    |  |
|  |     +----------------+             +----------------+            +-----------------+    |  |
|  |             |                              |                              |             |  |
|  +-----------------------------------------------------------------------------------------+  |
|                |                              |                              |                |
|                |                              |                              |                |
|  +---------------------------+  +---------------------------+  +---------------------------+  |
|  |             |             |  |             |             |  |             |             |  |
|  |  +---------------------+  |  |  +---------------------+  |  |	+---------------------+  |  |
|  |  | SERVICE (ClusterIP) +  |  |  | SERVICE (ClusterIP) |  |  |	| SERVICE (ClusterIP) |  |  |
|  |  +---------------------+  |  |  +---------------------+  |  |	+---------------------+  |  |
|  |     |       |       |     |  |     |       |       |     |  |     |       |       |     |  |
|  |     |       |       |     |  |     |       |       |     |  |     |       |       |     |  |
|  |     \       |       /     |  |     \       |       /     |  |     \       |       /     |  |
|  |  +-----+ +-----+ +-----+  |  |  +-----+ +-----+ +-----+  |  |	+-----+	+-----+	+-----+  |  |
|  |  | POD | | POD | | POD |  |  |  | POD | | POD | | POD |  |  |	| POD |	| POD |	| POD |  |  |
|  |  +-----+ +-----+ +-----+  |  |  +-----+ +-----+ +-----+  |  |	+-----+	+-----+ +-----+  |  |
|  |                           |  |                           |  |                           |  |
|  | WORKER NODE               |  | WORKER NODE               |  | WORKER NODE               |  |
|  +---------------------------+  +---------------------------+  +---------------------------+  |
|                                                                                               |
| KUBERNETES CLUSTER                                                                            |
+===============================================================================================+

##Create: NameSpaceVIPs, IP-Pools, Ingress, ClusterIPs, and Services

*kube-vip-cloud-provider
*A service will take an address based upon its namespace pool cidr/range-namespace

$ kubectl get configmap -n <$NAMESPACE> kubevip -o yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: dns-vip
  namespace: dns-pihole
data:
  cidr-default: 172.16.212.0/29
  cidr-copine-pihole01: 172.16.212.1/32
  cidr-copine-pihole02: 172.16.212.2/32
  cidr-<$APP99>: xxx.xxx.xxx.xxx/xx
  cidr-ipv6: xxxx::xx/xxx  #do not use IPv6 until deployed onto Production cluster

*create IP-Pools
Create an IP pool using a CIDR
kubectl create configmap --namespace kube-system kubevip --from-literal cidr-global=192.168.0.220/29

kubectl create configmap --namespace dns-pihole dns-pihole --from-literal cidr-global=172.16.212.0/29

255.255.254.0	COPINE K8 Cluster - serviceSubnets	10.0.212.0/23	10.0.212.1 - 10.0.213.254	10.0.213.255
255.255.255.0	cluster serviceSubnets	10.0.212.0/24	10.0.212.1 - 10.0.212.254	10.0.212.255
255.255.255.192	cluster 01 dev	10.0.212.0/26	10.0.212.1 - 10.0.212.62	10.0.212.63
255.255.255.192	cluster 02	10.0.212.64/26	10.0.212.65 - 10.0.212.126	10.0.212.127
255.255.255.192	cluster 03	10.0.212.128/26	10.0.212.129 - 10.0.212.190	10.0.212.191
255.255.255.192	cluster 04	10.0.212.192/26	10.0.212.193 - 10.0.212.254	10.0.212.255
255.255.255.0	cluster serviceSubnet	10.0.213.0/24	10.0.213.1 - 10.0.213.254	10.0.213.255
255.255.255.192	cluster 05	10.0.213.0/26	10.0.213.1 - 10.0.213.62	10.0.213.63
255.255.255.192	cluster 06	10.0.213.64/26	10.0.213.65 - 10.0.213.126	10.0.213.127
255.255.255.192	cluster 07	10.0.213.128/26	10.0.213.129 - 10.0.213.190	10.0.213.191
255.255.255.192	cluster 08	10.0.213.192/26	10.0.213.193 - 10.0.213.254	10.0.213.255
255.255.254.0	COPINE K8 Cluster01 - podSubnets	172.16.212.0/23	172.16.212.1 - 172.16.213.254	172.16.213.255
255.255.255.0	cluster podSubnets	172.16.212.0/24	172.16.212.1 - 172.16.212.254	172.16.212.255
255.255.255.192	cluster 01 dev	172.16.212.0/26	172.16.212.1 - 172.16.212.62	172.16.212.63
255.255.255.248	namespace 01 - pihole	172.16.212.0/29	172.16.212.1 - 172.16.212.6	172.16.212.7
	copine-pihole01		172.16.212.1	
	copine-pihole02		172.16.212.2	
				
255.255.255.192	cluster 02	172.16.212.64/26	172.16.212.65 - 172.16.212.126	172.16.212.127
255.255.255.192	cluster 03	172.16.212.128/26	172.16.212.129 - 172.16.212.190	172.16.212.191
255.255.255.192	cluster 04	172.16.212.192/26	172.16.212.193 - 172.16.212.254	172.16.212.255
255.255.255.0	podSubnet	172.16.213.0/24	172.16.213.1 - 172.16.213.254	172.16.213.255
255.255.255.192	cluster 05	172.16.213.0/26	172.16.213.1 - 172.16.213.62	172.16.213.63
255.255.255.192	cluster 06	172.16.213.64/26	172.16.213.65 - 172.16.213.126	172.16.213.127
255.255.255.192	cluster 07	172.16.213.128/26	172.16.213.129 - 172.16.213.190	172.16.213.191
255.255.255.192	cluster 08	172.16.213.192/26	172.16.213.193 - 172.16.213.254	172.16.213.255


	#refer to documentation on all deployment options for kubevip
	#https://github.com/kube-vip/kube-vip-cloud-provider

*ingress example
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80

*cluster IP example

*Create a file named cluster-ip-demo.yaml with the following:

apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  type: ClusterIP
  ports:
    - protocol: TCP
      targetPort: 80
      port: 80

*This will be our service. Currently, it's not bound to anything so let's create a sample pod.

*Create a file named pod-demo.yaml with the following:

apiVersion: v1
kind: Pod
metadata:
  name: demo-pod
  labels:
    app: demo-pod 
    type: frontend-app
spec:
  containers:
  - name: nginx-container
    image: nginx

*Now we can update our cluster-demo.yaml file to map our pods by their labels to the ClusterIP service.

apiVersion: v1
kind: Service
metadata:
  name: Backend
spec:
  type: ClusterIP
  ports:
    - targerPort: 80
      port: 80
  selector:
      name: demo-pod
      type: frontend-app
Now that we have configured the YAML files, we have to:

Create and run our pods.
Create and run our ClusterIP service.
To run the pod, use the following command:

kubectl apply -f pod-demo.yaml
Next, let's create our service. Run the following command:

kubectl create -f cluster-ip-demo.yaml
Now that the pods are running inside our cluster, we will expose it to external traffic using Kubernetes proxy.

Start the Kubernetes proxy using the following command:

kubernetes proxy --port=8080



###############################################################################
##
## COCloud Applications
##
##
##

*Unifi controller on kubernetes
*https://medium.com/@reefland/migrating-unifi-network-controller-from-docker-to-kubernetes-5aac8ed8da76
*https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/

*load balancing
https://github.com/kube-vip/kube-vip-cloud-provider




