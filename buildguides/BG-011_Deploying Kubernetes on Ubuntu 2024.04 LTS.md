# cocloud-k8s-dev-cluster-oogie

COCloud K8s Development Cluster Oogie

```text
      _
      _\___
    /     \
   | <> <> |
    \ ( ) /
     |||||
 \---/|||\---/
     |||||
     |||||
    _/   \_   
<------------->
```

## Table of Contents

1. [Introduction] (#intro)
2. [Development Environment] (#devenv)
   1. [Resource Specifications] (#resourcespec)
   2. [Assumptions & Dependencies] (#assumptdepend)
3. [Logical Topology] (#logicaltopology)
4. References and Notes
   1. Repositories
   2. Install nerdctl Command Line Tool
   3. Certificates
      1. Install Kubernetes Self-Signed CA
      2. Install Kubernetes Intermediate CA using third-party CA
   4. Prepare Internal Registry

## Introduction

Kubernetes, often abbreviated as K8s, is an open-source platform designed to automate the deployment, scaling, and operation of application containers. It provides a robust framework to run distributed systems resiliently, handling scaling and failover for Enterprise applications, and providing deployment patterns for developers. Whether you're managing a few containers or scaling to thousands, Kubernetes offers the tools and capabilities to ensure your applications run smoothly and efficiently.

In this guide, we'll walk you through the essential steps to set up, configure, and deploy a single application on Kubernetes using VXLAN network features. We'll start with setting up a Kubernetes cluster, followed by configuring networking, necessary components and resources, and finally deploying the application. By the end of this guide, you'll have a solid understanding of the Kubernetes architecture and be well-equipped to begin containerizing applications in an Enterprise production environment.

## Development Environment

- Resource Specifications
  - Physical Hardware
    - Dell Precision 3260
      - 13th Gen Intel(R) Core(TM) i9-13900 2.00 GHz
      - 32GB RAM
      - Windows 11 Enterprise 24H2
  - Hypervisor: Hyper-V
    - Compute
      - 2 vCPUs
      - 1GB/4GB RAM Start/Max
      - Ubuntu 24.04 LTS Minimal Installation
    - Networking
      - Host VLAN
        - Nodes CIDR: 10.0.69.32/27
      - Cluster VXLAN
        - Pods CIDR: 172.16.69.32/27
        - Service CIDR: 172.16.68.64/26
    - Storage
      - NFS Share Server
      - NFS Share Clients
- Assumptions & Dependencies
  - It is assumed that K8s is being deployed in an on-premises and offline laboratory environment to ensure strict configuration control.
  - It is assumed that all administration will be initiated or pushed to K8s cluster control and worker nodes. All commands should be done remotely unless directed otherwise.
  - It is assumed that the Intermediate Certificate Signing Request (CSR) generated by this deployment guide will be signed by an Enterprise Certificate Authority for K8s cluster certificate services.
  - It is assumed that static software and YAML files can be transferred from a bastion host into the offline environment.
  - It is assumed that all containers and images will be pulled from the Internet when the bastion host is connected to the external network, tagged with offline URL, then pushed to the offline registry.
  - It is assumed that all containers and images, when the bastion host is connected to the internal lab network, will be pulled from the offline registry.

## Logical Topology

```text
   +===============================================================================================+
   |                                                                                               |
   |  +-----------------------------------------------------------------------------------------+  |
   |  | INGRESS (kube-vip)                                                                      |  |
   |  |                                                                                         |  |
   |  |     +----------------+            +-----------------+            +-----------------+    |  |
   |  |     | api.domain.tld |            | app1.domain.ltd |            | app2.domain.ltd |    |  |
   |  |     +----------------+            +-----------------+            +-----------------+    |  |
   |  |             |                              |                              |             |  |
   |  +-----------------------------------------------------------------------------------------+  |
   |                |                              |                              |                |
   |                |                              |                              |                |
   |  +---------------------------+  +---------------------------+  +---------------------------+  |
   |  |             |             |  |             |             |  |             |             |  |
   |  |  +---------------------+  |  |  +---------------------+  |  |  +---------------------+  |  |
   |  |  | SERVICE (ClusterIP) +  |  |  | SERVICE (ClusterIP) |  |  |  | SERVICE (ClusterIP) |  |  |
   |  |  +---------------------+  |  |  +---------------------+  |  |  +---------------------+  |  |
   |  |     |       |       |     |  |     |       |       |     |  |     |       |       |     |  |
   |  |     |       |       |     |  |     |       |       |     |  |     |       |       |     |  |
   |  |     \       |       /     |  |     \       |       /     |  |     \       |       /     |  |
   |  |  +-----+ +-----+ +-----+  |  |  +-----+ +-----+ +-----+  |  |  +-----+ +-----+ +-----+  |  |
   |  |  | POD | | POD | | POD |  |  |  | POD | | POD | | POD |  |  |  | POD | | POD | | POD |  |  |
   |  |  +-----+ +-----+ +-----+  |  |  +-----+ +-----+ +-----+  |  |  +-----+ +-----+ +-----+  |  |
   |  |                           |  |                           |  |                           |  |
   |  | CONTROL NODE              |  | WORKER NODE               |  | WORKER NODE               |  |
   |  +---------------------------+  +---------------------------+  +---------------------------+  |
   |                                                                                               |
   | KUBERNETES CLUSTER                                                                            |
   +===============================================================================================+
```

## Tools, References, and Notes

### Repositories

```text
raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml
github.com/projectcalico/calico/releases/download/v3.29.1/calicoctl-linux-amd64
downloads.tigera.io/ee/binaries/v3.19.4/calicoctl
pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key
download.nerdctl.com/linux/ubuntu/gpg
download.nerdctl.com/linux/ubuntu
```

### References

<https://github.com/containerd/nerdctl>
<https://github.com/containerd/containerd/blob/main/docs/getting-started.md>

### Add Command Line Aliases

- For frequently used commands

```bash
tee ~/.bash_aliases <<EOF
alias k=kubectl
alias kg='kubectl get'
alias kd='kubectl describe'
alias ka='kubectl apply'
alias kdelf='kubectl delete -f'
alias kl='kubectl logs'
alias kgall='kubectl get all -A'
alias ktn='kubectl top node'
alias ktp='kubectl top pod'
alias n=nerdctl
alias nil='nerdctl image ls'
alias ncl='nerdctl container ls'
alias nirm='nerdctl image rm'
alias ncrm='nerdctl container rm'
alias ncstart='nerdctl container start'
alias ncstop='nerdctl container stop'
EOF

source ~/.bash_aliases
```

### Update OS and Install Dependencies

```bash
apt update && apt upgrade -y
apt install -y kubelet kubeadm kubectl vim apt-transport-https ca-certificates curl gpg net-tools gnupg
apt-mark hold kubelet kubeadm kubectl
apt update && apt upgrade -y
```

### Certificate Authority

Create the Kubernetes Cluster CA

```bash
  sudo -i
  mkdir /opt/ca
  mkdir /opt/ca/certs
  mkdir /opt/ca/crl
  mkdir /opt/ca/newcerts
  mkdir /opt/ca/private
  mkdir /opt/ca/requests
  touch /opt/ca/index.txt
  echo '1000' /opt/ca/serial
  chmod 600 /opt/ca
  cd /opt/ca
  openssl genrsa -aes256 -out private/cakey.pem 4096
  openssl req -new -x509 -key /opt/ca/cakey.pem -out cacert.pem -days 3650
  vi /usr/lib/ssl/openssl.cnf # [CA_default] dir = /opt/ca # Where everything is kept
  cd /opt/requests
```

#### Generate Kubernetes Cluster Intermediate Signing CA Certificate

- ca

```bash
openssl genrsa -out devca.key
openssl req -new -key devca.key -subj "/CN=Cantrell Cloud Kubernetes CA" -out devca.csr
  
sign the csr with Cantrell Cloud Signing CA
```

- create certificate request

```bash
  openssl req 
   -out offline-registry.csr 
   -newkey rsa:2048 
   -nodes 
   -keyout /opt/ca/private/offline-registry-key.pem 
   -extensions req_ext 
   -config offline-registry-san.cnf
```

- get certificate request signed by CA

```bash
  openssl ca \
   -in offline-registry.csr \
   -out offline-registry.crt \
   -extensions req_ext \
   -extfile offline-registry-san.cnf
```

- merge subca.crt and cacert.pem

```bash
  cat /opt/certs/offline-registry.crt \
  /opt/ca/cacert.pem > \
  /opt/ca/certs/offline-registry-chained.crt
```

### Create and Configure Offline Registry

```bash
nerdctl run -d -p 6000:5000 --restart always --name registry-airgapped registry:2
 1bf09802bee1476bc463d972c686f90a64640d87dacce1ac8485585de69c91a5
for image in `talosctl image default`; do nerdctl pull $image; done
for image in `talosctl image default`; do \
    nerdctl tag $image `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \
  done

for image in `talosctl image default`; do \
    nerdctl push `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \
    done

for image in $(cat talosctl-images.list) ; do \
    nerdctl tag $image `echo $image | sed -E 's#^[^/]+/#127.0.0.1:6000/#'`; \
  done
```

### Install Helm

```bash
  curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
  apt-get install apt-transport-https --yes
  echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
  apt-get update
  apt-get install helm
```

#### Install Helm Charts

- Pi-Hole helm chart
  
```bash
  helm repo add mojo2600 https://mojo2600.github.io/pihole-kubernetes/
  helm upgrade -i pihole mojo2600/pihole -f values-pihole.yaml
```

### Install NFS Shares

- On NFS Export server
  
```bash
  apt-get install nfs-kernel-server -y
  mkdir -p /opt/nfsshares
  mkdir -p /srv/nfs4/nfsshares
  mount --bind /opt/nfsshares /srv/nfs4/nfsshares
  vi /etc/fstab
    /opt/nfsshares /srv/nfs4/nfsshares none bind 0 0
  vi /etc/exports
    /shared/folder *(rw,no_root_squash,insecure,async,no_subtree_check,anonuid=1000,anongid=1000)
  exportfs -ar
  exportfs -v
  ufw allow 2049/tcp
  ufw allow 2049/udp
  reboot
```
  
- On each Worker Node
  
```bash
  apt-get install nfs-common -y
  mkdir /opt/nfsshares
  mount -t nfs -o vers=4 10.0.69.41:/srv/nfs4/nfsshares /opt/nfsshares
  df -h
  vi /etc/fstab
    10.0.69.41:/srv/nfs4/nfsserver /opt/nfsshares nfs defaults,timeo=900,retrans=5,_netdev 0 0
  reboot
```

### Install Linux Integration Services (Hyper-V Tools)

```bash
echo 'hv_vmbus' >> /etc/initramfs-tools/modules
echo 'hv_storvsc' >> /etc/initramfs-tools/modules
echo 'hv_blkvsc' >> /etc/initramfs-tools/modules
echo 'hv_netvsc' >> /etc/initramfs-tools/modules
apt update && apt upgrade -y && apt -y install linux-virtual linux-cloud-tools-virtual linux-tools-virtual
update-initramfs -u
reboot
```

-------------------------------------------------------------------------------

## Kubernetes cluster system configuration and initialization

### Verify the MAC address and product_uuid are unique for every node

You can get the MAC address of the network interfaces using the command

```bash
 ip link
 ```

 or

```bash
 ifconfig -a
```

### Network Configuration

- Set Interfaces - only if required

```bash
# The following will replace current network configuration
# Be sure to change required data before running tee command

tee /etc/netplan/50-cloud-init.yaml <<EOF
network:
    ethernets:
        eth0:
            addresses:
            - 10.0.69.51/27
            nameservers:
                addresses:
                - 172.16.69.71
                - 172.16.69.72
                search:
                - cantrellcloud.net
                - cantrelloffice.cloud
                - cantrell.cloud
            routes:
            -   to: default
                via: 10.0.69.33
        eth1: {}
    version: 2
    vlans:
        eth1.10:
            id: 10
            link: eth1
        eth1.101:
            id: 101
            link: eth1
EOF

netplan try
```

- The product_uuid can be checked by using the command

```bash
cat /sys/class/dmi/id/product_uuid
```

### Set NTP Client

```bash
# The following will replace current network configuration

tee /etc/systemd/timesyncd.conf <<EOF
[Time]
NTP=time.cantrelloffice.cloud
EOF

systemctl restart systemd-timesyncd
wait 1500
timedatectl timesync-status
```

### Update hosts File

```bash
vi /etc/hosts
```

```bash
10.0.69.50 kubeadmin.cantrellcloud.net
10.0.69.51 kubectrl01.cantrellcloud.net
10.0.69.52 kubectrl02.cantrellcloud.net
10.0.69.53 kubectrl03.cantrellcloud.net
10.0.69.56 kubework01.cantrellcloud.net
10.0.69.55 kubework02.cantrellcloud.net
10.0.69.56 kubework03.cantrellcloud.net
10.0.69.62 offline-registry
```

### Disable systemd-networkd-wait-online

```bash
/lib/systemd/system/systemd-networkd-wait-online.service

- add TimeoutStartSec=5sec under the [Service] section, e.g.:

[Service]
Type=oneshot
ExecStart=/lib/systemd/systemd-networkd-wait-online
RemainAfterExit=yes
TimeoutStartSec=5sec
```

### Turn off swap now and make persistent across reboots

```bash
swapoff-a
```

- Make Persistent across reboot; look for swap in --/etc/fstab and put a # in front of it

```bash
vi /etc/fstab
```

### Download public signing key for kubernetes repositories

```bash
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```

### add kubernetes apt repository

This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list

```bash
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list
```

### update apt index and install kubernetes packages

```bash
apt-get update
apt-get
```

### enable kubelet service

- the kubelet will restart every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.

```bash
systemctl enable --now kubelet
```

### Update firewall rules

- All Nodes

```bash
apt install -y ufw
```

- Control Plane rules

```bash
ufw allow 22/tcp
ufw allow 6443/tcp
ufw allow 2379/tcp
ufw allow 2380/tcp
ufw allow 8080/tcp
ufw allow 10248/tcp
ufw allow 10250/tcp
ufw allow 10259/tcp
ufw allow 10257/tcp
```

- Worker Nodes rules

```bash
ufw allow 22/tcp
ufw allow 5473/tcp
ufw allow 10250/tcp
ufw allow 10256/tcp
ufw allow 30000:32767/tcp
```

- All Nodes

```bash
ufw enable
```

- check firewall status

```bash
ufw status verbose
```

## Install ContainerD

- create containerd.conf

```bash
tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter
```

- create kube.conf

```bash
tee /etc/sysctl.d/kube.conf <<EOT
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOT

sysctl --system
```

### Install nerdctl Command Line Tool

- Download and extract nerdctl's binaries

```bash
cd $HOME
wget https://github.com/containerd/nerdctl/releases/download/v2.0.2/nerdctl-2.0.2-linux-amd64.tar.gz
tar Cxzvvf /usr/local/bin nerdctl-2.0.2-linux-amd64.tar.gz
```

- Enable/confirm cgroup v2

```bash
cat /etc/default/grub | grep systemd.unified_cgroup_hierarchy
```

- enable non-root cpu, cpuset, and i/o delegation - run as non-root user on k8s-controllers

```bash
sudo mkdir -p /etc/systemd/system/user@.service.d
cat <<EOF | sudo tee /etc/systemd/system/user@.service.d/delegate.conf
[Service]
Delegate=cpu cpuset io memory pids
EOF
sudo systemctl daemon-reload

cat /sys/fs/cgroup/user.slice/user-$(id -u).slice/user@$(id -u).service/cgroup.controllers  - run as non-root user on k8s-controllers
```

### Install Containerd

- Follow this for now:
  - <https://github.com/containerd/containerd/blob/main/docs/getting-started.md>

```bash
tar Cxzvf /usr/local containerd-x.x.x-linux-amd64.tar.gz

sudo install -m 755 runc.amd64 /usr/local/sbin/runc

mkdir -p /opt/cni/bin
sudo tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz

sudo cp containerd.service /usr/local/lib/systemd/system/

systemctl daemon-reload
systemctl enable --now containerd
```

configure the system so it starts using systemd as cgroup - still need to work on this

```bash
containerd config default | tee /etc/containerd/config.toml >/dev/null 2>&1
```

verify containerd config file - still need to work on this

```bash
cat /etc/containerd/config.toml
```

```text
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
SystemdCgroup = true
```

- Setup the service to start automatically and check to make sure it is running

```bash
systemctl restart containerd
systemctl enable containerd
systemctl status containerd
```

## Pull kubeadm config images and initialize default configuration

- Pull kubeadm default config - only on one of the controllers

```bash
sysctl --system
kubeadm config images pull
```

- Initialize default configuration
  - If building an image, this is a good time to take snapshot 1

```bash
kubeadm init
```

- Perform next commands as a regular user

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## Modify the kubelet

ConfigMap
Should be set, run command to verify cgroupDriver: systemd

```bash
kubectl edit cm kubelet-config -n kube-system
```

- Node Labels

```bash
kubectl label node kubectrl02 node-role.kubernetes.io/control-plane=
kubectl label node kubework01 node-role.kubernetes.io/worker=
```

## Install Calico network overlay

```bash
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml
kubectl create -f custom-resources.yaml
```

initialize overlay

```bash
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

may take a few minutes for all nodes in the cluster to spin up all the networking nodes and **report ready**

- You can watch by entering the following command

```bash
watch kubectl get pods -n calico-system
```

- Configure NSX overlay
  - It is best practice to use manifest (yaml) files which will be added in a future release
  - For now, create config.yaml files for each of the following IPPools to enable NSX overlay

```yaml
apiVersion: crd.projectcalico.org/v1
kind: IPPool
metadata:
  name: ippool-vxlan-dev-internal-subnets
  namespace: dev-internal
spec:
  allowedUses:
  - Workload
  - Tunnel
  blockSize: 26
  cidr: 192.168.69.0/24
  ipipMode: Always
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: CrossSubnet
```

```yaml
apiVersion: crd.projectcalico.org/v1
kind: IPPool
metadata:
  name: ippool-vxlan-dev-external-subnets
  namespace: dev-external
spec:
  allowedUses:
  - Workload
  - Tunnel
  blockSize: 26
  cidr: 192.168.68.0/24
  ipipMode: Always
  natOutgoing: true
  nodeSelector: all()
  vxlanMode: CrossSubnet
```

## Verify Kubernetes is running

```bash
kubectl get nodes
```

- If building an image, this is a good time to take snapshot 2

## Add additional nodes to cluster and labels, taints, and tolerances

- To join nodes, you must fist have a key
  - Copy/paste the displayed output to other nodes that are ready to be added to the cluster

```bash
kubeadm token create --print-join-command
```

- On each worker node as a regular user

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

- Label nodes

```bash
kubectl label node nodename key=value
```

- To bash into a pod

```bash
k exec --stdin --tty dev-intdmz-linux-test-65bf85b85d-lnnhw --namespace=dev-external -- /bin/bash
```
